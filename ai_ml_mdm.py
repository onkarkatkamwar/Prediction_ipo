# -*- coding: utf-8 -*-
"""AI-ML-MDM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TFLvp1CDPagqqyvDNMOd7_lnndF4unCJ

Name - Onkar Katkamwar

PRN - 202201040098

Batch - T(2) (Computer Enggineering)
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# Load the dataset
ipo_data = pd.read_csv('/content/sample_data/cleaned_ipo_data.csv')

# Display the first 5 rows
print(ipo_data.head())

print(ipo_data.isnull().sum())

# Step 1: Create Target Variable 'Pricing_Status'
ipo_data['Pricing_Status'] = ipo_data['Listing_Gain'].apply(lambda x: 'Underpriced' if x > 0 else 'Overpriced')

# Step 2: Encode categorical variables
ipo_data['Market_Profit'] = ipo_data['Market_Profit'].map({'Yes': 1, 'No': 0})
ipo_data['SME'] = ipo_data['SME'].map({'Yes': 1, 'No': 0})

# Display the first 5 rows after encoding
print("\nEncoded Data (First 5 Rows):")
print(ipo_data.head())

# Univariate Analysis: Numerical Data
numerical_columns = ['Issue_Price', 'Subscription_Rate', 'Listing_Gain']

# Check if the columns exist in the DataFrame
for col in numerical_columns:
    if col not in ipo_data.columns:
        print(f"Warning: Column '{col}' not found in the DataFrame. Skipping...")
        continue  # Skip to the next column if not found

    # Histogram
    plt.figure(figsize=(8, 4))
    sns.histplot(ipo_data[col], kde=True, color='blue')
    plt.title(f"Histogram of {col}")
    plt.show()

    # Distplot
    plt.figure(figsize=(8, 4))
    sns.kdeplot(ipo_data[col], shade=True, color='green')
    plt.title(f"Distribution Plot of {col}")
    plt.show()

    # Boxplot
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=ipo_data[col], color='purple')
    plt.title(f"Boxplot of {col}")
    plt.show()

# Scatter Plot: Issue Price vs Listing Gain
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Issue_price', y='Listing_Gain', hue='Market_Profit', data=ipo_data, palette='Set2')
plt.title("Issue Price vs Listing Gain")
plt.xlabel("Issue Price")
plt.ylabel("Listing Gain")
plt.legend(title="Market Profit")
plt.show()

# Bar Plot: SME vs Listing Gain
plt.figure(figsize=(8, 6))
sns.barplot(x='SME', y='Listing_Gain', data=ipo_data, ci=None, palette='coolwarm')
plt.title("SME vs Average Listing Gain")
plt.xlabel("SME (0 = No, 1 = Yes)")
plt.ylabel("Average Listing Gain")
plt.show()

# Box Plot: Market Profit vs Listing Gain
plt.figure(figsize=(8, 6))
sns.boxplot(x='Market_Profit', y='Listing_Gain', data=ipo_data, palette='pastel')
plt.title("Market Profit vs Listing Gain")
plt.xlabel("Market Profit (0 = No, 1 = Yes)")
plt.ylabel("Listing Gain")
plt.show()

# Drop unnecessary columns
ipo_data_cleaned = ipo_data.drop(columns=['Date', 'IPO_Name', 'Listing_Gain'])

# Separate features and target variable
X = ipo_data_cleaned.drop(columns=['Pricing_Status'])
y = ipo_data_cleaned['Pricing_Status'].map({'Underpriced': 1, 'Overpriced': 0})

# Step 3: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling
scaler = StandardScaler()

# Fit and transform the training data
X_train_scaled = scaler.fit_transform(X_train)

# Transform the test data
X_test_scaled = scaler.transform(X_test)

# Checking the effect of scaling
print("Mean of X_train after scaling:", np.mean(X_train_scaled, axis=0))
print("Standard deviation of X_train after scaling:", np.std(X_train_scaled, axis=0))

"""### Train Logistic Regression (LR) and Evaluate Performance"""

from sklearn.impute import SimpleImputer
from sklearn.metrics import confusion_matrix

# Step 1: Check for missing values
print("Missing values in the dataset:")
print(ipo_data_cleaned.isnull().sum())

# Step 2: Impute missing values in the features (X) using the median strategy
imputer = SimpleImputer(strategy='median')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Step 3: Train Logistic Regression model on the imputed data
lr_model = LogisticRegression()

# Train the model
lr_model.fit(X_train_imputed, y_train)

# Predict on the test set
y_pred_lr = lr_model.predict(X_test_imputed)

# Calculate accuracy
accuracy_lr = accuracy_score(y_test, y_pred_lr)

# Print the results
print("Logistic Regression Model Performance:")
print(f"Accuracy: {accuracy_lr:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred_lr))
print("Confusion Matrix:")
conf_matrix = confusion_matrix(y_test, y_pred_lr)
print(conf_matrix)

# Interpretation: If accuracy > 0.5, we can say IPO is underpriced, otherwise overpriced
if accuracy_lr > 0.5:
    print("The IPO is predicted to be underpriced.")
else:
    print("The IPO is predicted to be overpriced.")

"""### Steps for K-Nearest Neighbors (KNN):"""

# Step 1: Initialize K-Nearest Neighbors model
from sklearn.neighbors import KNeighborsClassifier

knn_model = KNeighborsClassifier(n_neighbors=5)  # You can change the number of neighbors if needed

# Step 2: Train the KNN model
knn_model.fit(X_train_imputed, y_train)

# Step 3: Predict on the test set
y_pred_knn = knn_model.predict(X_test_imputed)

# Step 4: Calculate accuracy
accuracy_knn = accuracy_score(y_test, y_pred_knn)

# Print the results
print("\nK-Nearest Neighbors Model Performance:")
print(f"Accuracy: {accuracy_knn:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred_knn))
print("Confusion Matrix:")
conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)
print(conf_matrix_knn)

# Interpretation: If accuracy > 0.5, we can say IPO is underpriced, otherwise overpriced
if accuracy_knn > 0.5:
    print("The IPO is predicted to be underpriced.")
else:
    print("The IPO is predicted to be overpriced.")

"""### Steps for Decision Tree Model:"""

# Step 1: Initialize Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier()

# Step 2: Train the Decision Tree model
dt_model.fit(X_train_imputed, y_train)

# Step 3: Predict on the test set
y_pred_dt = dt_model.predict(X_test_imputed)

# Step 4: Calculate accuracy
accuracy_dt = accuracy_score(y_test, y_pred_dt)

# Print the results
print("\nDecision Tree Model Performance:")
print(f"Accuracy: {accuracy_dt:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred_dt))
print("Confusion Matrix:")
conf_matrix_dt = confusion_matrix(y_test, y_pred_dt)
print(conf_matrix_dt)

# Interpretation: If accuracy > 0.5, we can say IPO is underpriced, otherwise overpriced
if accuracy_dt > 0.5:
    print("The IPO is predicted to be underpriced.")
else:
    print("The IPO is predicted to be overpriced.")

"""### Steps for Support Vector Classifier (SVC):"""

# Step 1: Initialize Support Vector Classifier
from sklearn.svm import SVC

svc_model = SVC(probability=True)

# Step 2: Train the SVC model
svc_model.fit(X_train_imputed, y_train)

# Step 3: Predict on the test set
y_pred_svc = svc_model.predict(X_test_imputed)

# Step 4: Calculate accuracy
accuracy_svc = accuracy_score(y_test, y_pred_svc)

# Print the results
print("\nSupport Vector Classifier Model Performance:")
print(f"Accuracy: {accuracy_svc:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred_svc))
print("Confusion Matrix:")
conf_matrix_svc = confusion_matrix(y_test, y_pred_svc)
print(conf_matrix_svc)

# Interpretation: If accuracy > 0.5, we can say IPO is underpriced, otherwise overpriced
if accuracy_svc > 0.5:
    print("The IPO is predicted to be underpriced.")
else:
    print("The IPO is predicted to be overpriced.")

"""### Steps for Random Forest (RF):"""

# Step 1: Initialize Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier()

# Step 2: Train the Random Forest model
rf_model.fit(X_train_imputed, y_train)

# Step 3: Predict on the test set
y_pred_rf = rf_model.predict(X_test_imputed)

# Step 4: Calculate accuracy
accuracy_rf = accuracy_score(y_test, y_pred_rf)

# Print the results
print("\nRandom Forest Model Performance:")
print(f"Accuracy: {accuracy_rf:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred_rf))
print("Confusion Matrix:")
conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)
print(conf_matrix_rf)

# Interpretation: If accuracy > 0.5, we can say IPO is underpriced, otherwise overpriced
if accuracy_rf > 0.5:
    print("The IPO is predicted to be underpriced.")
else:
    print("The IPO is predicted to be overpriced.")

"""## Compare the performance of all models:"""

# Step 1: Store all model results (accuracy scores)
model_accuracies = {
    "K-Nearest Neighbors": accuracy_knn,
    "Decision Tree": accuracy_dt,
    "Support Vector Classifier": accuracy_svc,
    "Random Forest": accuracy_rf
}

# Step 2: Print out accuracy for all models
print("\nModel Accuracy Comparison:")
for model, accuracy in model_accuracies.items():
    print(f"{model}: Accuracy = {accuracy:.2f}")

# Step 3: Conclusion based on highest accuracy
best_model = max(model_accuracies, key=model_accuracies.get)
print(f"\nBest Performing Model: {best_model} with Accuracy: {model_accuracies[best_model]:.2f}")

"""### Prediction by taking Input from User"""

import numpy as np

# Sample input for a new IPO (you can change these values as per the new IPO data)
new_ipo_data = {
    'Issue_Size(crores)': 500,         # Example: 500 crores
    'QIB': 35,                         # Example: 35%
    'HNI': 25,                         # Example: 25%
    'RII': 40,                         # Example: 40%
    'Issue_price': 120,                # Example: 120 INR
    'Listing_Open': 125,               # Example: 125 INR
    'Listing_Close': 130,              # Example: 130 INR
    'CMP': 140,                        # Example: 140 INR
    'Market_Profit': 1,                # Example: 1 (Yes)
    'SME': 0                           # Example: 0 (No)
}

# Convert the input data into a DataFrame (matching the feature columns)
new_ipo_df = pd.DataFrame([new_ipo_data])

# Apply any preprocessing steps: encoding, scaling
# Encoding categorical variables (Market_Profit and SME)
new_ipo_df['Market_Profit'] = new_ipo_df['Market_Profit'].map({'Yes': 1, 'No': 0})
new_ipo_df['SME'] = new_ipo_df['SME'].map({'Yes': 1, 'No': 0})

# Feature scaling (using the same scaler from the training data)
new_ipo_scaled = scaler.transform(new_ipo_df)

# Make prediction using the best model (replace 'rf_model' with your best performing model)
new_ipo_prediction = rf_model.predict(new_ipo_scaled)

# Output the result: whether the IPO is predicted to be underpriced (1) or overpriced (0)
if new_ipo_prediction[0] == 1:
    print("The IPO is predicted to be underpriced.")
else:
    print("The IPO is predicted to be overpriced.")